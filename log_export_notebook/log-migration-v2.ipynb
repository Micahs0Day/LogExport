{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install azure-monitor-query"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from azure.identity import AzureCliCredential\n",
        "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from azure.core.exceptions import ResourceExistsError\n",
        "import time\n",
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"ExportPipeline\")\n",
        "\n",
        "# Azure auth (Service Principal)\n",
        "credential = AzureCliCredential()\n",
        "\n",
        "# Clients\n",
        "workspace_id = \"<la_workspace_id>\"\n",
        "logs_client = LogsQueryClient(credential)\n",
        "storage_account_name = \"<storage_account_name>\"\n",
        "account_url = f\"https://<storage_account_name>.blob.core.windows.net\"\n",
        "blob_service_client = BlobServiceClient(\n",
        "    account_url, \n",
        "    credential=credential,\n",
        "    retry_total=5, retry_connect=5, retry_read=5, retry_status=5\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1753293444349
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_table(table_name, start_time, end_time, time_chunk):\n",
        "    logger.info(f\"Starting export for {table_name}\")\n",
        "    # Ensure container exists\n",
        "    container_name = table_name.lower()\n",
        "    container_name = re.sub(r'[^a-zA-Z0-9\\s]', '', container_name)\n",
        "    try:\n",
        "        container_client = blob_service_client.create_container(container_name)\n",
        "    except ResourceExistsError:\n",
        "        container_client = blob_service_client.get_container_client(container_name)\n",
        "    # Collect data in chunks\n",
        "    current = start_time\n",
        "    all_chunks = []\n",
        "    while current < end_time:\n",
        "        next_time = min(current + time_chunk, end_time)\n",
        "        kql = f\"{table_name} | where TimeGenerated between (startofday(datetime({current.isoformat()})) .. startofday(datetime({next_time.isoformat()})))\"\n",
        "        logger.info(f\"Querying {table_name} from {current} to {next_time}\")\n",
        "        # Simple retry logic for query\n",
        "        for attempt in range(3):\n",
        "            try:\n",
        "                resp = logs_client.query_workspace(workspace_id, query=kql, timespan=(current, next_time))\n",
        "                break\n",
        "            except Exception as err:\n",
        "                logger.warning(f\"Query attempt {attempt+1} failed: {err}\")\n",
        "                time.sleep(2 ** attempt)\n",
        "        else:\n",
        "            logger.error(f\"All query attempts failed for range {current} - {next_time}; skipping\")\n",
        "            current = next_time\n",
        "            continue\n",
        "        # Handle response\n",
        "        if resp.status == LogsQueryStatus.SUCCESS:\n",
        "            tables = resp.tables\n",
        "        else:\n",
        "            logger.warning(f\"Partial result for {table_name} at {current}: {resp.partial_error}\")\n",
        "            tables = resp.partial_data\n",
        "        # Convert to DataFrame\n",
        "        for table in tables:\n",
        "            df_chunk = pd.DataFrame(data=table.rows, columns=table.columns)\n",
        "            all_chunks.append(df_chunk)\n",
        "        current = next_time\n",
        "    if not all_chunks:\n",
        "        logger.info(f\"No data for {table_name}\")\n",
        "        return\n",
        "    df_table = pd.concat(all_chunks, ignore_index=True)\n",
        "    logger.info(f\"Exported {len(df_table)} rows for {table_name}\")\n",
        "    # Upload DataFrame as JSON\n",
        "    json_bytes = df_table.to_json(orient='records', lines=True).encode('utf-8')\n",
        "    blob_name = f\"{table_name}_{start_time.date()}_{end_time.date()}.json\"\n",
        "    blob_client = container_client.get_blob_client(blob=blob_name)\n",
        "    # Retry on upload\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            blob_client.upload_blob(io.BytesIO(json_bytes), overwrite=True)\n",
        "            logger.info(f\"Uploaded {blob_name} to container {container_name}\")\n",
        "            break\n",
        "        except Exception as err:\n",
        "            logger.warning(f\"Upload attempt {attempt+1} failed: {err}\")\n",
        "            time.sleep(2 ** attempt)\n",
        "    else:\n",
        "        logger.error(f\"Failed to upload {blob_name} after retries\")\n",
        "\n",
        "# List of tables and ranges to export\n",
        "tables = [\"<table1>\",\"<table_2>\"]\n",
        "# (Year, Month, Day)\n",
        "# Starts at the beginning of this day\n",
        "start_time = datetime(2025, 7, 14)\n",
        "# Ends before the start of this day\n",
        "end_time   = datetime(2025, 7, 22)\n",
        "# How often to query data\n",
        "time_chunk = timedelta(days=1)\n",
        "\n",
        "# Parallel export\n",
        "with ThreadPoolExecutor(max_workers=len(tables)) as executor:\n",
        "    futures = [\n",
        "        executor.submit(export_table, tbl, start_time, end_time, time_chunk)\n",
        "        for tbl in tables\n",
        "    ]\n",
        "    for f in as_completed(futures):\n",
        "        if f.exception():\n",
        "            logger.error(f\"Error in export: {f.exception()}\")\n",
        "\n",
        "logger.info(\"All exports completed.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1753293463428
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}