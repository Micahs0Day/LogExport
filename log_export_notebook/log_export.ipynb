{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fded40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time\n",
    "import io\n",
    "import gzip\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from msticpy.data import QueryProvider\n",
    "\n",
    "# ----------------------------\n",
    "# üîß CONFIGURATION SECTION\n",
    "# ----------------------------\n",
    "\n",
    "# Load environment variables (like secrets)\n",
    "load_dotenv()\n",
    "\n",
    "# Azure storage account connection string from .env\n",
    "STORAGE_CONNECTION_STRING = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "\n",
    "# Define the time range for queries\n",
    "START_TIME = dt.datetime.utcnow() - dt.timedelta(days=365)\n",
    "END_TIME = dt.datetime.utcnow()\n",
    "\n",
    "# List of Log Analytics tables to export\n",
    "TABLES = [\"SecurityEvent\", \"Syslog\", \"SigninLogs\"]\n",
    "\n",
    "# How big to start each chunk (can shrink if needed)\n",
    "INITIAL_CHUNK_HOURS = 1\n",
    "MIN_CHUNK_MINUTES = 5\n",
    "MAX_ROWS = 500_000  # hard cap enforced by Log Analytics\n",
    "\n",
    "# Number of times to retry failed blob uploads\n",
    "RETRY_LIMIT = 3\n",
    "\n",
    "# Number of tables to export in parallel\n",
    "MAX_THREADS = 3\n",
    "\n",
    "# Where to save the metadata of each export\n",
    "METADATA_LOG = \"export_metadata.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# üßæ LOGGING SETUP\n",
    "# ----------------------------\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# ----------------------------\n",
    "# üîå DATA SOURCE SETUP (MSTICPy)\n",
    "# ----------------------------\n",
    "\n",
    "qry_prov = QueryProvider(\"AzureMonitor\")  # Assumes you are authenticated with Azure\n",
    "\n",
    "# ----------------------------\n",
    "# ‚òÅÔ∏è STORAGE CLIENT SETUP\n",
    "# ----------------------------\n",
    "\n",
    "blob_service_client = BlobServiceClient.from_connection_string(STORAGE_CONNECTION_STRING)\n",
    "\n",
    "# Collect metadata for final report\n",
    "metadata_entries = []\n",
    "\n",
    "# ----------------------------\n",
    "# üìä Query a time window of data from a table\n",
    "# ----------------------------\n",
    "def run_kql(table, start_time, end_time):\n",
    "    query = f\"{table} | where TimeGenerated between (datetime('{start_time}') .. datetime('{end_time}'))\"\n",
    "    df = qry_prov.exec_query(query)\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# üì§ Upload a blob with retry/backoff\n",
    "# ----------------------------\n",
    "def upload_blob_with_retry(container_client, blob_name, data, retry_limit=RETRY_LIMIT):\n",
    "    for attempt in range(1, retry_limit + 1):\n",
    "        try:\n",
    "            container_client.upload_blob(blob_name, data, overwrite=True)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"[Attempt {attempt}] Upload failed: {e}\")\n",
    "            time.sleep(2 ** attempt)  # exponential backoff\n",
    "    return False  # fail after max retries\n",
    "\n",
    "# ----------------------------\n",
    "# üîÅ Export all records from a single table\n",
    "# ----------------------------\n",
    "def export_table(table):\n",
    "    logger.info(f\"Starting export for table: {table}\")\n",
    "    container_client = blob_service_client.get_container_client(table.lower())\n",
    "    container_client.create_container(exist_ok=True)  # create if not already there\n",
    "\n",
    "    current_start = START_TIME\n",
    "    chunk_hours = INITIAL_CHUNK_HOURS\n",
    "\n",
    "    while current_start < END_TIME:\n",
    "        current_end = min(current_start + dt.timedelta(hours=chunk_hours), END_TIME)\n",
    "\n",
    "        # Initialize metadata entry for this batch\n",
    "        log_entry = {\n",
    "            \"table\": table,\n",
    "            \"start_time\": current_start.isoformat(),\n",
    "            \"end_time\": current_end.isoformat(),\n",
    "            \"rows\": 0,\n",
    "            \"blob_name\": None,\n",
    "            \"success\": False,\n",
    "            \"duration_sec\": 0\n",
    "        }\n",
    "\n",
    "        logger.info(f\"[{table}] Querying: {current_start} ‚Üí {current_end} ({chunk_hours}h)\")\n",
    "        t0 = time.time()\n",
    "\n",
    "        try:\n",
    "            df = run_kql(table, current_start, current_end)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"[{table}] Query failed: {e}\")\n",
    "            current_start = current_end\n",
    "            continue\n",
    "\n",
    "        duration = time.time() - t0\n",
    "        log_entry[\"rows\"] = len(df)\n",
    "        log_entry[\"duration_sec\"] = round(duration, 2)\n",
    "\n",
    "        # If results hit the cap, reduce chunk size and retry\n",
    "        if len(df) >= MAX_ROWS:\n",
    "            if chunk_hours * 60 <= MIN_CHUNK_MINUTES:\n",
    "                logger.warning(f\"[{table}] Chunk too large even at min size. Skipping: {current_start}\")\n",
    "                current_start = current_end\n",
    "            else:\n",
    "                chunk_hours /= 2\n",
    "            continue\n",
    "\n",
    "        if len(df) > 0:\n",
    "            # Serialize the data to GZipped JSON\n",
    "            blob_name = f\"{table}_{current_start:%Y%m%dT%H%M}_{current_end:%Y%m%dT%H%M}.json.gz\"\n",
    "            log_entry[\"blob_name\"] = blob_name\n",
    "\n",
    "            json_bytes = io.BytesIO()\n",
    "            with gzip.GzipFile(fileobj=json_bytes, mode='w') as f:\n",
    "                f.write(df.to_json(orient='records', lines=True).encode())\n",
    "            json_bytes.seek(0)\n",
    "\n",
    "            # Upload the blob\n",
    "            success = upload_blob_with_retry(container_client, blob_name, json_bytes)\n",
    "            log_entry[\"success\"] = success\n",
    "\n",
    "            if success:\n",
    "                logger.info(f\"[{table}] Uploaded {blob_name}\")\n",
    "            else:\n",
    "                logger.error(f\"[{table}] Upload failed for {blob_name}\")\n",
    "\n",
    "        current_start = current_end\n",
    "        metadata_entries.append(log_entry)\n",
    "\n",
    "# ----------------------------\n",
    "# üöÄ Main loop to run all exports in parallel\n",
    "# ----------------------------\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "        futures = [executor.submit(export_table, table) for table in TABLES]\n",
    "        for future in as_completed(futures):\n",
    "            future.result()  # force exceptions to be raised if any\n",
    "\n",
    "    # Save metadata to CSV\n",
    "    df_log = pd.DataFrame(metadata_entries)\n",
    "    df_log.to_csv(METADATA_LOG, index=False)\n",
    "    logger.info(f\"Export metadata saved to: {METADATA_LOG}\")\n",
    "\n",
    "# ----------------------------\n",
    "# üîß Entry point\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
