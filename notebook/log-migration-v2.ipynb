{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install azure-monitor-query"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting azure-monitor-query\n  Downloading azure_monitor_query-1.4.1-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: azure-core>=1.28.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-monitor-query) (1.35.0)\nRequirement already satisfied: isodate>=0.6.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-monitor-query) (0.7.2)\nRequirement already satisfied: typing-extensions>=4.0.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-monitor-query) (4.14.1)\nRequirement already satisfied: requests>=2.21.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-core>=1.28.0->azure-monitor-query) (2.32.4)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from azure-core>=1.28.0->azure-monitor-query) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-monitor-query) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-monitor-query) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-monitor-query) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-monitor-query) (2025.7.9)\nDownloading azure_monitor_query-1.4.1-py3-none-any.whl (157 kB)\nInstalling collected packages: azure-monitor-query\nSuccessfully installed azure-monitor-query-1.4.1\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from azure.identity import AzureCliCredential, ManagedIdentityCredential\n",
        "from azure.monitor.query import LogsQueryClient, LogsQueryStatus\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "from azure.core.exceptions import ResourceExistsError\n",
        "import time\n",
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(\"ExportPipeline\")\n",
        "\n",
        "# Azure auth (Service Principal)\n",
        "credential = ManagedIdentityCredential()\n",
        "\n",
        "# Clients\n",
        "workspace_id = \"<log_analytics_workspace_id>\"\n",
        "logs_client = LogsQueryClient(credential)\n",
        "storage_account_name = \"<storage_account_name>\"\n",
        "account_url = f\"https://{storage_account_name}.blob.core.windows.net\"\n",
        "blob_service_client = BlobServiceClient(\n",
        "    account_url, \n",
        "    credential=credential,\n",
        "    retry_total=5, retry_connect=5, retry_read=5, retry_status=5\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "INFO:azure.identity._credentials.managed_identity:ManagedIdentityCredential will use Azure ML managed identity\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1753307255776
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_table(table_name, start_time, end_time, time_chunk):\n",
        "    logger.info(f\"Starting export for {table_name}\")\n",
        "    # Ensure container exists\n",
        "    container_name = table_name.lower()\n",
        "    container_name = re.sub(r'[^a-zA-Z0-9\\s]', '', container_name)\n",
        "    try:\n",
        "        container_client = blob_service_client.create_container(container_name)\n",
        "    except ResourceExistsError:\n",
        "        container_client = blob_service_client.get_container_client(container_name)\n",
        "    # Collect data in chunks\n",
        "    current = start_time\n",
        "    all_chunks = []\n",
        "    while current < end_time:\n",
        "        next_time = min(current + time_chunk, end_time)\n",
        "        kql = f\"{table_name} | where TimeGenerated between (startofday(datetime({current.isoformat()})) .. startofday(datetime({next_time.isoformat()})))\"\n",
        "        logger.info(f\"Querying {table_name} from {current} to {next_time}\")\n",
        "        # Simple retry logic for query\n",
        "        for attempt in range(3):\n",
        "            try:\n",
        "                resp = logs_client.query_workspace(workspace_id, query=kql, timespan=(current, next_time))\n",
        "                break\n",
        "            except Exception as err:\n",
        "                logger.warning(f\"Query attempt {attempt+1} failed: {err}\")\n",
        "                time.sleep(2 ** attempt)\n",
        "        else:\n",
        "            logger.error(f\"All query attempts failed for range {current} - {next_time}; skipping\")\n",
        "            current = next_time\n",
        "            continue\n",
        "        # Handle response\n",
        "        if resp.status == LogsQueryStatus.SUCCESS:\n",
        "            tables = resp.tables\n",
        "        else:\n",
        "            logger.warning(f\"Partial result for {table_name} at {current}: {resp.partial_error}\")\n",
        "            tables = resp.partial_data\n",
        "        # Convert to DataFrame\n",
        "        for table in tables:\n",
        "            df_chunk = pd.DataFrame(data=table.rows, columns=table.columns)\n",
        "            all_chunks.append(df_chunk)\n",
        "        current = next_time\n",
        "    if not all_chunks:\n",
        "        logger.info(f\"No data for {table_name}\")\n",
        "        return\n",
        "    df_table = pd.concat(all_chunks, ignore_index=True)\n",
        "    logger.info(f\"Exported {len(df_table)} rows for {table_name}\")\n",
        "    # Upload DataFrame as JSON\n",
        "    json_bytes = df_table.to_json(orient='records', lines=True).encode('utf-8')\n",
        "    blob_name = f\"{table_name}_{start_time.date()}_{end_time.date()}.json\"\n",
        "    blob_client = container_client.get_blob_client(blob=blob_name)\n",
        "    # Retry on upload\n",
        "    for attempt in range(3):\n",
        "        try:\n",
        "            blob_client.upload_blob(io.BytesIO(json_bytes), overwrite=True)\n",
        "            logger.info(f\"Uploaded {blob_name} to container {container_name}\")\n",
        "            break\n",
        "        except Exception as err:\n",
        "            logger.warning(f\"Upload attempt {attempt+1} failed: {err}\")\n",
        "            time.sleep(2 ** attempt)\n",
        "    else:\n",
        "        logger.error(f\"Failed to upload {blob_name} after retries\")\n",
        "\n",
        "# List of tables and ranges to export\n",
        "tables = [\"<table_1>\",\"<table_2>\"]\n",
        "# (Year, Month, Day)\n",
        "# Starts at the beginning of this day\n",
        "start_time = datetime(2025, 7, 14)\n",
        "# Ends before the start of this day\n",
        "end_time   = datetime(2025, 7, 22)\n",
        "# How often to query data\n",
        "time_chunk = timedelta(days=1)\n",
        "\n",
        "# Parallel export\n",
        "with ThreadPoolExecutor(max_workers=len(tables)) as executor:\n",
        "    futures = [\n",
        "        executor.submit(export_table, tbl, start_time, end_time, time_chunk)\n",
        "        for tbl in tables\n",
        "    ]\n",
        "    for f in as_completed(futures):\n",
        "        if f.exception():\n",
        "            logger.error(f\"Error in export: {f.exception()}\")\n",
        "\n",
        "logger.info(\"All exports completed.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "INFO:ExportPipeline:Starting export for ISSAAPI_CL\nINFO:azure.identity._internal.msal_managed_identity_client:AzureMLCredential.get_token_info succeeded\nINFO:azure.identity._internal.decorators:ManagedIdentityCredential.get_token_info succeeded\nINFO:ExportPipeline:Querying ISSAAPI_CL from 2025-07-14 00:00:00 to 2025-07-15 00:00:00\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nINFO:azure.identity._internal.msal_managed_identity_client:AzureMLCredential.get_token_info succeeded\nINFO:azure.identity._internal.decorators:ManagedIdentityCredential.get_token_info succeeded\nINFO:ExportPipeline:Querying ISSAAPI_CL from 2025-07-15 00:00:00 to 2025-07-16 00:00:00\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nINFO:ExportPipeline:Querying ISSAAPI_CL from 2025-07-16 00:00:00 to 2025-07-17 00:00:00\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nINFO:ExportPipeline:Querying ISSAAPI_CL from 2025-07-17 00:00:00 to 2025-07-18 00:00:00\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nINFO:ExportPipeline:Querying ISSAAPI_CL from 2025-07-18 00:00:00 to 2025-07-19 00:00:00\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nINFO:ExportPipeline:Querying ISSAAPI_CL from 2025-07-19 00:00:00 to 2025-07-20 00:00:00\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nINFO:ExportPipeline:Querying ISSAAPI_CL from 2025-07-20 00:00:00 to 2025-07-21 00:00:00\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nINFO:ExportPipeline:Querying ISSAAPI_CL from 2025-07-21 00:00:00 to 2025-07-22 00:00:00\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nWARNING:azure.monitor.query._generated._serialization:Datetime with no tzinfo will be considered UTC.\nINFO:ExportPipeline:Exported 5826 rows for ISSAAPI_CL\nINFO:ExportPipeline:Uploaded ISSAAPI_CL_2025-07-14_2025-07-22.json to container issaapicl\nINFO:ExportPipeline:All exports completed.\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1753307266547
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}